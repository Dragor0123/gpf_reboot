# GPF Reboot - Target-Centric Prior Modeling Configuration with SVD
# Datasets : ['cora', 'citeseer', 'pubmed', 'computers', 'photo']
# Experiment Type & Cross-Domain Settings
experiment:
  type: "cross_domain"  # "single_domain" or "cross_domain"
  source_dataset: "cora"
  target_dataset: "computers"
  seed: 42
  device: auto
  log_level: INFO
  run_name: null  # auto-generated if null

# Dataset Configuration
dataset:
  # For single_domain experiments
  name: "cora"  # used when experiment.type == "single_domain"
  
  # Cross-domain split ratios
  split:
    val_ratio: 0.1
    test_ratio: 0.2
    shuffle: true
  
  # Data loading settings
  data_loading:
    cache_datasets: true  # cache loaded datasets

# SVD Feature Reduction (NEW!)
feature_reduction:
  enable: true
  method: "svd"  # currently only "svd" supported
  target_dim: 100  # all datasets reduced to 100D
  save_reducer: true  # save SVD parameters for reuse
  explained_variance_threshold: 0.95  # minimum variance to preserve

# Model Architecture
# model:
#   type: gin
#   hidden_dim: 128
#   num_layers: 5
#   dropout: 0.5

model:
  type: "gcnii"
  hidden_dim: 128
  num_layers: 5      # 깊게 설정 가능
  dropout: 0.5
  alpha: 0.2         # 적당한 initial feature 보존
  theta: 1.0         # 표준 identity mapping

# Pretraining (on source dataset with SVD)
pretrain:
  lr: 0.001
  weight_decay: 0.0005
  epochs: 1000
  
  # Contrastive learning settings
  augmentation:
    view1: dropN
    view2: permE
    aug_ratio: 0.2
    temperature: 0.5

# Prompt Tuning (on target dataset with SVD alignment)
prompt_tuning:
  lr: 0.01
  weight_decay: 0.0005
  epochs: 200
  early_stopping:
    enable: true
    patience: 30
    min_delta: 0.001

# Prompt Configuration
prompt:
  type: "residual_mlp"  # "gpf", "residual_mlp", "linear"
  
  # GPFPrompt parameters (when type: "gpf")
  gpf:
    num_prompts: 10
  
  # ResidualMLPPrompt parameters (when type: "residual_mlp")
  residual_mlp:
    hidden_dim: 64
    num_layers: 2
    dropout: 0.1
  
  # LinearPrompt parameters (when type: "linear")
  linear:
    out_channels: null  # null means same as input_dim (in_channels)

target_centric:
  enable: true
  regularization:
    beta: 0.1 #0.1
    anchor:
      type: "gradient_optimized"  # "gaussian", "mog", "gradient_optimized"
      num_anchors: 1000           # Total number of anchors to generate
      
      # MoG-specific parameters (when type: "mog")
      num_components: 8      # Gaussian component 수 (6개?)
      use_sklearn_gmm: true  # true면 full GMM, false면 simplified
      
      # Gradient optimization parameters (when type: "gradient_optimized")
      optimization:
        learning_rate: 0.01      # Learning rate for gradient ascent
        num_iterations: 100      # Number of optimization iterations
        regularization_lambda: 0.1  # Regularization strength
        objectives:
          high_norm: 0.3         # Encourage strong activations
          diversity: 0.4         # Maximize embedding diversity
          task_alignment: 0.3    # Align with downstream task
          graph_homophily: 0.0   # Preserve graph structure (0.0 = disabled)
    mapper:
      type: "identity"       # MoG/gradient는 이미 latent space에서 생성
    divergence:
      type: "mmd"
      params:
        sigma: 0.25

# Evaluation & Logging
evaluation:
  metrics: ["accuracy", "f1_macro", "f1_micro"]
  save_results: true
  results_dir: "results"
  
# Reproducibility
reproducibility:
  deterministic: true
  benchmark: false