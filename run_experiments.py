#!/usr/bin/env python3
"""
Target-Centric Prior Modeling ì „ì²´ ì¡°í•© ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸

ì‹¤í—˜ ì„¤ê³„:
1. GPF Baseline (target_centric.enable = false)
2. GPF + Target-Centric Prior Modeling (target_centric.enable = true)

ëª¨ë“  ê°€ëŠ¥í•œ source-target ì¡°í•©ì—ì„œ ì„±ëŠ¥ ë¹„êµ (5*4 = 20ê°€ì§€)
"""

import subprocess
import yaml
import logging
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple
import pandas as pd
import itertools
from datetime import datetime
import json


# ì‹¤í—˜ ì„¤ì •
DATASETS = ['cora', 'citeseer', 'pubmed', 'computer', 'photo']
REGULARIZATION_TYPES = ['mmd', 'entropy', 'manifold']
BETA_VALUES = [0.01, 0.1, 0.5, 1.0]

# ëª¨ë“  ê°€ëŠ¥í•œ í¬ë¡œìŠ¤ ë„ë©”ì¸ ì¡°í•© ìƒì„± (source != target)
def generate_all_cross_domain_pairs(datasets: List[str]) -> List[Tuple[str, str]]:
    """
    ëª¨ë“  ê°€ëŠ¥í•œ source-target ì¡°í•© ìƒì„±
    
    Args:
        datasets: ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        (source, target) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    """
    pairs = []
    for source in datasets:
        for target in datasets:
            if source != target:  # sourceì™€ targetì´ ë‹¤ë¥¸ ê²½ìš°ë§Œ
                pairs.append((source, target))
    return pairs


CROSS_DOMAIN_PAIRS = generate_all_cross_domain_pairs(DATASETS)


def setup_experiment_logging():
    """ì‹¤í—˜ ë¡œê¹… ì„¤ì •"""
    log_filename = f"experiments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler()
        ]
    )
    
    logging.info(f"Experiment log file: {log_filename}")


def create_config(source_dataset: str, target_dataset: str, 
                 target_centric_enabled: bool = False, 
                 regularization_type: str = 'mmd',
                 beta: float = 0.1) -> Dict[str, Any]:
    """
    ì‹¤í—˜ìš© ì„¤ì • ìƒì„±
    
    Args:
        source_dataset: ì†ŒìŠ¤ ë°ì´í„°ì…‹
        target_dataset: íƒ€ê²Ÿ ë°ì´í„°ì…‹
        target_centric_enabled: Target-centric í™œì„±í™” ì—¬ë¶€
        regularization_type: ì •ê·œí™” íƒ€ì…
        beta: ì •ê·œí™” ê°€ì¤‘ì¹˜
    
    Returns:
        ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    config = {
        'experiment': {
            'type': 'cross_domain',
            'source_dataset': source_dataset,
            'target_dataset': target_dataset,
            'seed': 42,
            'device': 'auto',
            'log_level': 'INFO'
        },
        'dataset': {
            'split': {
                'val_ratio': 0.1,
                'test_ratio': 0.2,
                'shuffle': True
            }
        },
        'model': {
            'type': 'gin',
            'hidden_dim': 128,
            'num_layers': 5,
            'dropout': 0.5
        },
        'pretrain': {
            'lr': 0.001,
            'weight_decay': 0.0005,
            'epochs': 1000,
            'save_path': f'checkpoints/{source_dataset}_encoder.pt',
            'augmentation': {
                'view1': 'dropN',
                'view2': 'permE',
                'aug_ratio': 0.2,
                'temperature': 0.5
            }
        },
        'prompt_tuning': {
            'lr': 0.01,
            'weight_decay': 0.0005,
            'epochs': 200,
            'early_stopping': {
                'enable': True,
                'patience': 30,
                'min_delta': 0.001
            }
        },
        'prompt': {
            'type': 'gpf_plus',
            'num_prompts': 10
        },
        'target_centric': {
            'enable': target_centric_enabled,
            'regularization': {
                'type': regularization_type,
                'beta': beta,
                'mmd': {
                    'sigma': 1.0,
                    'anchor_type': 'random',
                    'num_anchors': 100
                },
                'entropy': {
                    'temperature': 1.0
                },
                'manifold': {
                    'neighbor_k': 5
                }
            }
        },
        'evaluation': {
            'metrics': ['accuracy', 'f1_macro', 'f1_micro'],
            'save_results': True,
            'results_dir': 'results'
        }
    }
    
    return config


def save_config(config: Dict[str, Any], filepath: str):
    """ì„¤ì •ì„ YAML íŒŒì¼ë¡œ ì €ì¥"""
    with open(filepath, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)


def run_single_experiment(config: Dict[str, Any], experiment_name: str) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰
    
    Args:
        config: ì‹¤í—˜ ì„¤ì •
        experiment_name: ì‹¤í—˜ ì´ë¦„
    
    Returns:
        ì‹¤í—˜ ê²°ê³¼
    """
    logging.info(f"ğŸš€ Starting experiment: {experiment_name}")
    
    # ì„¤ì • íŒŒì¼ ì €ì¥
    config_path = f"config_temp_{experiment_name}.yaml"
    save_config(config, config_path)
    
    try:
        # í˜„ì¬ config.yamlì„ ë°±ì—…
        if Path("config.yaml").exists():
            subprocess.run(["cp", "config.yaml", "config_backup.yaml"], check=True)
        
        # ì„ì‹œ ì„¤ì •ì„ ë©”ì¸ ì„¤ì •ìœ¼ë¡œ ë³µì‚¬
        subprocess.run(["cp", config_path, "config.yaml"], check=True)
        
        # ì‚¬ì „í›ˆë ¨ ì‹¤í–‰ (source ë°ì´í„°ì…‹ì—ì„œ)
        logging.info(f"   Running pretraining on {config['experiment']['source_dataset']}...")
        
        start_time = time.time()
        result = subprocess.run(
            ["python", "train_pretrain.py"], 
            capture_output=True, 
            text=True,
            timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
        )
        
        if result.returncode != 0:
            raise RuntimeError(f"Pretraining failed: {result.stderr}")
        
        pretrain_time = time.time() - start_time
        logging.info(f"   Pretraining completed in {pretrain_time:.1f}s")
        
        # í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹¤í–‰ (target ë°ì´í„°ì…‹ì—ì„œ)
        logging.info(f"   Running prompt tuning on {config['experiment']['target_dataset']}...")
        
        start_time = time.time()
        result = subprocess.run(
            ["python", "train_prompt_tuning.py"], 
            capture_output=True, 
            text=True,
            timeout=1800  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        if result.returncode != 0:
            raise RuntimeError(f"Prompt tuning failed: {result.stderr}")
        
        tuning_time = time.time() - start_time
        logging.info(f"   Prompt tuning completed in {tuning_time:.1f}s")
        
        # ê²°ê³¼ íŒŒì‹± ì‹œë„
        test_metrics = parse_results_from_output(result.stdout)
        
        logging.info(f"âœ… Completed experiment: {experiment_name}")
        if test_metrics:
            logging.info(f"   Test Accuracy: {test_metrics.get('accuracy', 'N/A'):.4f}")
        
        return {
            'status': 'success',
            'experiment_name': experiment_name,
            'pretrain_time': pretrain_time,
            'tuning_time': tuning_time,
            'total_time': pretrain_time + tuning_time,
            'test_metrics': test_metrics
        }
        
    except subprocess.TimeoutExpired:
        logging.error(f"âŒ Experiment {experiment_name} timed out")
        return {
            'status': 'timeout',
            'experiment_name': experiment_name,
            'error': 'Experiment timed out'
        }
        
    except Exception as e:
        logging.error(f"âŒ Failed experiment {experiment_name}: {str(e)}")
        return {
            'status': 'failed',
            'experiment_name': experiment_name,
            'error': str(e)
        }
    
    finally:
        # ì„¤ì • íŒŒì¼ ì •ë¦¬
        Path(config_path).unlink(missing_ok=True)
        
        # ë°±ì—…ëœ ì„¤ì • ë³µì›
        if Path("config_backup.yaml").exists():
            subprocess.run(["mv", "config_backup.yaml", "config.yaml"], check=False)


def parse_results_from_output(output: str) -> Dict[str, float]:
    """
    ì‹¤í–‰ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ íŒŒì‹±
    """
    metrics = {}
    lines = output.split('\n')
    
    # "FINAL TEST RESULTS" ì„¹ì…˜ ì°¾ê¸°
    in_results_section = False
    
    for line in lines:
        if "FINAL TEST RESULTS" in line:
            in_results_section = True
            continue
            
        if in_results_section and "=" in line and "=" * 10 in line:
            in_results_section = False
            continue
            
        if in_results_section and ":" in line:
            try:
                parts = line.split(":")
                if len(parts) == 2:
                    metric_name = parts[0].strip().lower()
                    metric_value = float(parts[1].strip())
                    metrics[metric_name] = metric_value
            except (ValueError, IndexError):
                continue
    
    return metrics


def run_baseline_vs_target_centric():
    """
    ëª¨ë“  ì¡°í•©ì—ì„œ Baseline vs Target-Centric Prior Modeling ë¹„êµ ì‹¤í—˜
    """
    logging.info("ğŸ”¬ Starting comprehensive Baseline vs Target-Centric comparison")
    logging.info(f"ğŸ“Š Total combinations: {len(CROSS_DOMAIN_PAIRS)} (5Ã—4 = 20)")
    
    results = []
    
    for i, (source, target) in enumerate(CROSS_DOMAIN_PAIRS, 1):
        logging.info(f"\n{'='*60}")
        logging.info(f"ğŸ“Š Combination {i}/{len(CROSS_DOMAIN_PAIRS)}: {source} â†’ {target}")
        logging.info(f"{'='*60}")
        
        # 1. Baseline ì‹¤í—˜ (Target-Centric ë¹„í™œì„±í™”)
        logging.info(f"ğŸ”µ Running Baseline GPF...")
        baseline_config = create_config(
            source_dataset=source,
            target_dataset=target,
            target_centric_enabled=False
        )
        
        baseline_name = f"baseline_{source}_to_{target}"
        baseline_result = run_single_experiment(baseline_config, baseline_name)
        baseline_result.update({
            'experiment_type': 'baseline',
            'source': source,
            'target': target,
            'target_centric': False
        })
        results.append(baseline_result)
        
        # 2. Target-Centric ì‹¤í—˜ (MMD ì •ê·œí™”)
        logging.info(f"ğŸ¯ Running Target-Centric GPF...")
        target_centric_config = create_config(
            source_dataset=source,
            target_dataset=target,
            target_centric_enabled=True,
            regularization_type='mmd',
            beta=0.1
        )
        
        target_centric_name = f"target_centric_{source}_to_{target}"
        target_centric_result = run_single_experiment(target_centric_config, target_centric_name)
        target_centric_result.update({
            'experiment_type': 'target_centric',
            'source': source,
            'target': target,
            'target_centric': True,
            'regularization_type': 'mmd',
            'beta': 0.1
        })
        results.append(target_centric_result)
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ì‹¤í—˜ ì¤‘ë‹¨ ì‹œ ë³µêµ¬ìš©)
        save_intermediate_results(results, f"intermediate_results_{i}.json")
        
        # ì ì‹œ ëŒ€ê¸° (ì‹œìŠ¤í…œ ì•ˆì •ì„±)
        time.sleep(3)
    
    return results


def run_regularization_ablation():
    """
    ì •ê·œí™” íƒ€ì…ë³„ ablation study (ì„ íƒëœ ì¡°í•©ì—ì„œë§Œ)
    """
    logging.info("ğŸ”¬ Starting regularization ablation study")
    
    # ëŒ€í‘œì ì¸ ë„ë©”ì¸ ì¡°í•© ì„ íƒ (ë„ˆë¬´ ë§ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    selected_pairs = [
        ('cora', 'computer'),    # Citation â†’ Amazon
        ('computer', 'cora'),    # Amazon â†’ Citation
        ('citeseer', 'photo'),   # Citation â†’ Amazon
        ('photo', 'citeseer'),   # Amazon â†’ Citation
        ('pubmed', 'computer')   # Citation â†’ Amazon
    ]
    
    logging.info(f"ğŸ“Š Ablation on {len(selected_pairs)} representative pairs")
    
    results = []
    
    for source, target in selected_pairs:
        logging.info(f"\nğŸ“Š Ablation study: {source} â†’ {target}")
        
        for reg_type in REGULARIZATION_TYPES:
            # ê° ì •ê·œí™” íƒ€ì…ë§ˆë‹¤ ì¤‘ê°„ beta ê°’ í•˜ë‚˜ë§Œ í…ŒìŠ¤íŠ¸
            beta = 0.1
            
            config = create_config(
                source_dataset=source,
                target_dataset=target,
                target_centric_enabled=True,
                regularization_type=reg_type,
                beta=beta
            )
            
            experiment_name = f"ablation_{source}_to_{target}_{reg_type}_beta{beta}"
            result = run_single_experiment(config, experiment_name)
            
            result.update({
                'experiment_type': 'ablation',
                'source': source,
                'target': target,
                'target_centric': True,
                'regularization_type': reg_type,
                'beta': beta
            })
            results.append(result)
            
            time.sleep(2)
    
    return results


def save_intermediate_results(results: List[Dict[str, Any]], filename: str):
    """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
    with open(filename, 'w') as f:
        json.dump(results, f, indent=2, default=str)


def analyze_results(results: List[Dict[str, Any]]):
    """
    ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ìš”ì•½
    """
    logging.info("ğŸ“ˆ Analyzing experimental results...")
    
    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(results)
    
    if len(df) == 0:
        logging.warning("No results to analyze")
        return
    
    # ì„±ê³µí•œ ì‹¤í—˜ë§Œ í•„í„°ë§
    successful_df = df[df['status'] == 'success'].copy()
    
    logging.info(f"Successful experiments: {len(successful_df)}/{len(df)}")
    
    # ê¸°ë³¸ í†µê³„
    logging.info("\n" + "="*60)
    logging.info("ğŸ“Š EXPERIMENT SUMMARY")
    logging.info("="*60)
    logging.info(f"Total experiments: {len(df)}")
    logging.info(f"Successful: {len(successful_df)}")
    logging.info(f"Failed: {len(df[df['status'] == 'failed'])}")
    logging.info(f"Timeout: {len(df[df['status'] == 'timeout'])}")
    
    if len(successful_df) > 0:
        # Baseline vs Target-Centric ë¹„êµ
        baseline_df = successful_df[successful_df['experiment_type'] == 'baseline']
        tc_df = successful_df[successful_df['experiment_type'] == 'target_centric']
        
        if len(baseline_df) > 0 and len(tc_df) > 0:
            logging.info("\nğŸ“Š BASELINE vs TARGET-CENTRIC COMPARISON:")
            
            # í‰ê·  ì„±ëŠ¥
            baseline_acc = [r.get('accuracy', 0) for r in baseline_df['test_metrics'] if r]
            tc_acc = [r.get('accuracy', 0) for r in tc_df['test_metrics'] if r]
            
            if baseline_acc and tc_acc:
                baseline_mean = sum(baseline_acc) / len(baseline_acc)
                tc_mean = sum(tc_acc) / len(tc_acc)
                
                logging.info(f"Baseline Average Accuracy: {baseline_mean:.4f}")
                logging.info(f"Target-Centric Average Accuracy: {tc_mean:.4f}")
                
                improvement = (tc_mean - baseline_mean) / baseline_mean * 100 if baseline_mean > 0 else 0
                logging.info(f"Average Improvement: {improvement:+.2f}%")
                
                # ê°œë³„ ì¡°í•©ë³„ ê²°ê³¼
                logging.info("\nğŸ“‹ DETAILED RESULTS BY TRANSFER PAIR:")
                
                for source in DATASETS:
                    for target in DATASETS:
                        if source == target:
                            continue
                            
                        baseline_result = baseline_df[
                            (baseline_df['source'] == source) & 
                            (baseline_df['target'] == target)
                        ]
                        tc_result = tc_df[
                            (tc_df['source'] == source) & 
                            (tc_df['target'] == target)
                        ]
                        
                        if len(baseline_result) > 0 and len(tc_result) > 0:
                            baseline_metrics = baseline_result.iloc[0]['test_metrics']
                            tc_metrics = tc_result.iloc[0]['test_metrics']
                            
                            if baseline_metrics and tc_metrics:
                                baseline_acc = baseline_metrics.get('accuracy', 0)
                                tc_acc = tc_metrics.get('accuracy', 0)
                                
                                if baseline_acc > 0:
                                    improvement = (tc_acc - baseline_acc) / baseline_acc * 100
                                    status = "ğŸŸ¢" if improvement > 0 else "ğŸ”´"
                                    
                                    logging.info(f"{status} {source:>8} â†’ {target:<8}: "
                                               f"Baseline={baseline_acc:.4f}, "
                                               f"TC={tc_acc:.4f}, "
                                               f"Î”={improvement:+.2f}%")
    
    # ì‹¤íŒ¨í•œ ì‹¤í—˜ ë¡œê¹…
    failed_df = df[df['status'] != 'success']
    if len(failed_df) > 0:
        logging.info("\nâš ï¸  FAILED EXPERIMENTS:")
        for _, row in failed_df.iterrows():
            logging.info(f"   {row['experiment_name']}: {row.get('error', 'Unknown error')}")
    
    logging.info("="*60)


def main():
    """
    ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜
    """
    setup_experiment_logging()
    
    logging.info("ğŸ§ª Starting Comprehensive Target-Centric Prior Modeling Experiments")
    logging.info(f"ğŸ“… Experiment started at: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    logging.info(f"ğŸ”¢ Total cross-domain combinations: {len(CROSS_DOMAIN_PAIRS)}")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    Path("results").mkdir(exist_ok=True)
    Path("checkpoints").mkdir(exist_ok=True)
    
    all_results = []
    
    try:
        # 1. ëª¨ë“  ì¡°í•©ì—ì„œ Baseline vs Target-Centric ë¹„êµ
        logging.info("\n" + "="*80)
        logging.info("PHASE 1: Comprehensive Baseline vs Target-Centric Comparison")
        logging.info("="*80)
        
        baseline_results = run_baseline_vs_target_centric()
        all_results.extend(baseline_results)
        
        # 2. ì„ íƒëœ ì¡°í•©ì—ì„œ ì •ê·œí™” íƒ€ì… ablation study
        logging.info("\n" + "="*80)
        logging.info("PHASE 2: Regularization Ablation Study")
        logging.info("="*80)
        
        ablation_results = run_regularization_ablation()
        all_results.extend(ablation_results)
        
        # 3. ê²°ê³¼ ë¶„ì„
        logging.info("\n" + "="*80)
        logging.info("PHASE 3: Results Analysis")
        logging.info("="*80)
        
        analyze_results(all_results)
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON í˜•ì‹ìœ¼ë¡œ ìƒì„¸ ê²°ê³¼ ì €ì¥
        results_json = f"results/comprehensive_results_{timestamp}.json"
        with open(results_json, 'w') as f:
            json.dump(all_results, f, indent=2, default=str)
        
        # CSV í˜•ì‹ìœ¼ë¡œ ìš”ì•½ ì €ì¥
        results_df = pd.DataFrame(all_results)
        results_csv = f"results/comprehensive_summary_{timestamp}.csv"
        results_df.to_csv(results_csv, index=False)
        
        logging.info("ğŸ‰ All experiments completed successfully!")
        logging.info(f"ğŸ“ Detailed results: {results_json}")
        logging.info(f"ğŸ“Š Summary CSV: {results_csv}")
        
    except KeyboardInterrupt:
        logging.info("ğŸ›‘ Experiment interrupted by user")
        logging.info("ğŸ’¾ Saving partial results...")
        
        if all_results:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            partial_results = f"results/partial_results_{timestamp}.json"
            with open(partial_results, 'w') as f:
                json.dump(all_results, f, indent=2, default=str)
            logging.info(f"ğŸ“ Partial results saved: {partial_results}")
        
    except Exception as e:
        logging.error(f"âŒ Experiment suite failed: {str(e)}")
        raise
    
    finally:
        logging.info(f"ğŸ Experiment ended at: {time.strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main()