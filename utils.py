"""
utils.py
ê¸°ë³¸ì ì¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ê³¼ ì„¤ì • ê´€ë ¨ í•¨ìˆ˜ë“¤
"""

import os
import random
import logging
import torch
import numpy as np
import yaml
from pathlib import Path
from typing import Dict, Any, Optional
import yaml


def set_seed(seed: int = 42):
    """
    ëª¨ë“  ëžœë¤ ì‹œë“œ ê³ ì • (ìž¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ì„ ìœ„í•´)
    ì›ë³¸ GPFì˜ ì‹œë“œ ì„¤ì • ë°©ì‹ ì°¸ê³ 
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # ì¶”ê°€ì ì¸ CUDA ìž¬í˜„ì„± ì„¤ì •
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def get_device(device_str: str = "auto") -> torch.device:
    """
    ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ê²°ì •
    
    Args:
        device_str: "auto", "cuda", "cpu" ì¤‘ í•˜ë‚˜
        
    Returns:
        torch.device ê°ì²´
    """
    if device_str == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    elif device_str == "cuda":
        if not torch.cuda.is_available():
            print("âš ï¸  CUDA not available, falling back to CPU")
            device = torch.device("cpu")
        else:
            device = torch.device("cuda")
    elif device_str == "cpu":
        device = torch.device("cpu")
    else:
        raise ValueError(f"Unknown device: {device_str}")
    
    print(f"ðŸ–¥ï¸  Using device: {device}")
    return device


def load_config(config_path: str = "config.yaml") -> Dict[str, Any]:
    """
    YAML ì„¤ì • íŒŒì¼ ë¡œë“œ
    
    Args:
        config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    config_file = Path(config_path)
    
    if not config_file.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")
    
    with open(config_file, 'r', encoding='utf-8') as f:
        # yaml.safe_load ëŒ€ì‹  yaml.load with SafeLoader ì‚¬ìš©
        # ì´ë ‡ê²Œ í•˜ë©´ ê³¼í•™ì  í‘œê¸°ë²•ì´ ì˜¬ë°”ë¥´ê²Œ íŒŒì‹±ë©ë‹ˆë‹¤
        config = yaml.load(f, Loader=yaml.SafeLoader)
    
    print(f"ðŸ“ Loaded config from: {config_path}")
    return config


def setup_logging(log_level: str = "INFO", log_file: Optional[str] = None):
    """
    ë¡œê¹… ì„¤ì •
    
    Args:
        log_level: ë¡œê·¸ ë ˆë²¨ ("DEBUG", "INFO", "WARNING", "ERROR")
        log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ì½˜ì†”ì—ë§Œ ì¶œë ¥)
    """
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    level = getattr(logging, log_level.upper())
    
    # ë¡œê·¸ í¬ë§·
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # í•¸ë“¤ëŸ¬ ì„¤ì •
    handlers = []
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    handlers.append(console_handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ì„ íƒì )
    if log_file:
        log_path = Path(log_file).parent
        log_path.mkdir(parents=True, exist_ok=True)
        
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        handlers.append(file_handler)
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    logging.basicConfig(
        level=level,
        handlers=handlers,
        force=True  # ê¸°ì¡´ ì„¤ì • ë®ì–´ì“°ê¸°
    )


def count_parameters(model: torch.nn.Module) -> int:
    """
    ëª¨ë¸ì˜ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    
    Args:
        model: PyTorch ëª¨ë¸
        
    Returns:
        í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜
    """
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def save_ckpt(model: torch.nn.Module, 
                   optimizer: torch.optim.Optimizer,
                   epoch: int,
                   loss: float,
                   filepath: str,
                   **kwargs):
    """
    ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ìž¥
    
    Args:
        model: ì €ìž¥í•  ëª¨ë¸
        optimizer: ì˜µí‹°ë§ˆì´ì €
        epoch: í˜„ìž¬ ì—í¬í¬
        loss: í˜„ìž¬ ì†ì‹¤ê°’
        filepath: ì €ìž¥í•  íŒŒì¼ ê²½ë¡œ
        **kwargs: ì¶”ê°€ ì €ìž¥í•  ì •ë³´
    """
    checkpoint_dir = Path(filepath).parent
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        **kwargs
    }
    
    torch.save(checkpoint, filepath)
    print(f"ðŸ’¾ Checkpoint saved: {filepath}")


def load_ckpt(filepath: str, 
                   model: torch.nn.Module,
                   optimizer: Optional[torch.optim.Optimizer] = None,
                   device: Optional[torch.device] = None) -> Dict[str, Any]:
    """
    ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    
    Args:
        filepath: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        model: ë¡œë“œí•  ëª¨ë¸
        optimizer: ì˜µí‹°ë§ˆì´ì € (ì„ íƒì )
        device: ë””ë°”ì´ìŠ¤ (ì„ íƒì )
        
    Returns:
        ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    if not Path(filepath).exists():
        raise FileNotFoundError(f"Checkpoint not found: {filepath}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    map_location = device if device else torch.device('cpu')
    
    checkpoint = torch.load(filepath, map_location=map_location)
    
    # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ (ì„ íƒì )
    if optimizer and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    print(f"ðŸ“‚ Checkpoint loaded: {filepath}")
    print(f"   - Epoch: {checkpoint.get('epoch', 'Unknown')}")
    print(f"   - Loss: {checkpoint.get('loss', 'Unknown')}")
    
    return checkpoint


def create_run_name(config: Dict[str, Any]) -> str:
    """
    ì‹¤í—˜ ì‹¤í–‰ ì´ë¦„ ìƒì„± (ë¡œê¹… ë° ì²´í¬í¬ì¸íŠ¸ìš©)
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ì‹¤í–‰ ì´ë¦„ ë¬¸ìžì—´
    """
    dataset = config.get('dataset', {}).get('name', 'unknown')
    model = config.get('model', {}).get('type', 'unknown')
    prompt = config.get('prompt', {}).get('type', 'none')
    
    run_name = f"{dataset}_{model}_{prompt}"
    
    # ì¶”ê°€ íŒŒë¼ë¯¸í„°ë“¤
    if prompt != 'none':
        num_prompts = config.get('prompt', {}).get('num_prompts')
        if num_prompts:
            run_name += f"_p{num_prompts}"
    
    return run_name


def print_model_info(model: torch.nn.Module, model_name: str = "Model"):
    """
    ëª¨ë¸ ì •ë³´ ì¶œë ¥
    
    Args:
        model: PyTorch ëª¨ë¸
        model_name: ëª¨ë¸ ì´ë¦„
    """
    total_params = count_parameters(model)
    
    print(f"\nðŸ“Š {model_name} Information:")
    print(f"   - Total parameters: {total_params:,}")
    print(f"   - Model size: {total_params * 4 / 1024 / 1024:.2f} MB")
    
    # ê° ëª¨ë“ˆë³„ íŒŒë¼ë¯¸í„° ìˆ˜
    print(f"   - Module breakdown:")
    for name, module in model.named_children():
        module_params = count_parameters(module)
        print(f"     - {name}: {module_params:,}")


def ensure_dir(directory: str):
    """
    ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    
    Args:
        directory: ìƒì„±í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    Path(directory).mkdir(parents=True, exist_ok=True)


def get_git_hash() -> Optional[str]:
    """
    í˜„ìž¬ Git commit hash ê°€ì ¸ì˜¤ê¸° (ìž¬í˜„ì„±ì„ ìœ„í•´)
    
    Returns:
        Git commit hash ë˜ëŠ” None
    """
    try:
        import subprocess
        result = subprocess.run(['git', 'rev-parse', 'HEAD'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            return result.stdout.strip()[:8]  # ì§§ì€ í•´ì‹œ
    except:
        pass
    return None


def log_experiment_info(config: Dict[str, Any]):
    """
    ì‹¤í—˜ ì •ë³´ ë¡œê¹…
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    logger = logging.getLogger(__name__)
    
    logger.info("ðŸ§ª Experiment Information:")
    logger.info(f"   - Dataset: {config.get('dataset', {}).get('name', 'Unknown')}")
    logger.info(f"   - Model: {config.get('model', {}).get('type', 'Unknown')}")
    logger.info(f"   - Prompt: {config.get('prompt', {}).get('type', 'None')}")
    logger.info(f"   - Device: {config.get('experiment', {}).get('device', 'auto')}")
    logger.info(f"   - Seed: {config.get('experiment', {}).get('seed', 42)}")
    
    # Git ì •ë³´
    git_hash = get_git_hash()
    if git_hash:
        logger.info(f"   - Git commit: {git_hash}")


class EarlyStopping:
    """
    Early Stopping í´ëž˜ìŠ¤
    ì›ë³¸ GPFì˜ ì¡°ê¸° ì¢…ë£Œ ë°©ì‹ ì°¸ê³ 
    """
    
    def __init__(self, patience: int = 10, min_delta: float = 0.0, mode: str = 'max'):
        """
        Args:
            patience: ê°œì„ ì´ ì—†ì„ ë•Œ ê¸°ë‹¤ë¦´ ì—í¬í¬ ìˆ˜
            min_delta: ê°œì„ ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ë³€í™”ëŸ‰
            mode: 'max' (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ) ë˜ëŠ” 'min' (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.wait = 0
        self.best_score = None
        self.early_stop = False
        
    def __call__(self, score: float) -> bool:
        """
        Args:
            score: í˜„ìž¬ ì„±ëŠ¥ ì ìˆ˜
            
        Returns:
            ì¡°ê¸° ì¢…ë£Œ ì—¬ë¶€
        """
        if self.best_score is None:
            self.best_score = score
        elif self._is_improvement(score):
            self.best_score = score
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.early_stop = True
                
        return self.early_stop
    
    def _is_improvement(self, score: float) -> bool:
        """ê°œì„  ì—¬ë¶€ íŒë‹¨"""
        if self.mode == 'max':
            return score > self.best_score + self.min_delta
        else:
            return score < self.best_score - self.min_delta
        
        