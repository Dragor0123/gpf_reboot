# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a modern implementation of Graph Prompt Feature (GPF) for cross-domain graph neural network transfer learning, featuring **Dynamic Performance-Based Anchor Selection**. The project focuses on learning transferable representations across different graph datasets using contrastive pretraining, SVD feature alignment, and novel dynamic anchor-based regularization.

### Key Innovation: Dynamic Anchor Selection

Unlike traditional static anchor methods (e.g., MoG-based), this implementation features:
- **Performance-driven anchor selection**: High-performing nodes serve as reference anchors
- **Dynamic adaptation**: Anchor quality improves during training
- **Source-free operation**: No dependency on source domain data
- **Self-supervised discovery**: Target domain provides its own reference signals

## Key Commands

### Environment Setup
```bash
pip install -r requirements.txt
```

### Running Experiments
```bash
# Single cross-domain experiment with dynamic anchors
python train_pretrain.py    # Pretrain encoder on source dataset
python train_fine_tuning.py # Fine-tune on target with dynamic anchor selection

# Comprehensive evaluation across all dataset pairs
python run_experiments.py   # Runs cross-domain experiments with dynamic vs static anchor comparison

# Debug and validation
python debug_data_check.py     # Verify dataset loading and SVD alignment
python debug_step_by_step.py   # Step-by-step debugging of the pipeline
python debug_anchor_selection.py  # Debug dynamic anchor selection process
```

### Configuration

All experiments are controlled via `config.yaml` with structured validation:

#### Core Experiment Settings
```yaml
experiment:
  type: "cross_domain"  # or "single_domain"
  source_dataset: "cora"
  target_dataset: "computers"

feature_reduction:
  enable: true  # SVD dimensionality reduction (recommended)
  target_dim: 100
```

#### Dynamic Anchor Selection (Primary Method)
```yaml
dynamic_anchor:
  enable: true
  selection_ratio: 0.2          # Top 20% performing nodes
  update_frequency: 10          # Update every 10 epochs
  criteria:
    loss_weight: 0.7           # Primary: individual node loss
    confidence_weight: 0.3     # Secondary: prediction confidence
  quality_tracking: true       # Track anchor quality metrics
  fallback:
    enable: true
    method: "mog"             # Fallback to MoG if dynamic fails
    conditions:
      performance_drop: 0.05   # 5% accuracy drop triggers fallback
      anchor_diversity: 0.3    # Minimum diversity threshold
      selection_instability: 0.5  # Minimum overlap between updates

target_centric:
  enable: true
  regularization:
    weight: 0.1               # Regularization strength
```

#### Legacy Methods (Available but Deprecated)
```yaml
mog_anchor:
  enable: false               # Disabled when dynamic_anchor is true
  num_components: 8
  num_anchors: 1000

static_anchor:
  enable: false               # Legacy static anchor selection
```

## Technical Architecture

### Core Dynamic Anchor Components

#### File Structure (To Be Implemented)
```
core/
â”œâ”€â”€ dynamic_anchor_selector.py    # Core dynamic anchor selection logic
â”œâ”€â”€ performance_evaluator.py      # Node performance evaluation
â”œâ”€â”€ anchor_quality_tracker.py     # Quality metrics and monitoring
â””â”€â”€ config.py                     # Enhanced configuration management

training/
â”œâ”€â”€ dynamic_regularizers.py       # Dynamic anchor-based regularization
â”œâ”€â”€ losses.py                     # Enhanced loss functions
â””â”€â”€ trainer.py                    # Training loop with dynamic anchors
```

#### Key Classes (Design Phase)

**DynamicPerformanceAnchorSelector**:
```python
class DynamicPerformanceAnchorSelector:
    """Core component for performance-based anchor selection"""
    
    def __init__(self, selection_ratio=0.2, update_frequency=10):
        self.selection_ratio = selection_ratio
        self.update_frequency = update_frequency
        self.performance_history = []
    
    def select_anchors_by_performance(self, embeddings, logits, labels, mask, epoch):
        """Select anchors based on current node performance"""
        # Implementation planned
        
    def compute_anchor_quality_metrics(self, anchors, all_embeddings):
        """Track anchor diversity, representativeness, stability"""
        # Implementation planned
```

**NodePerformanceEvaluator**:
```python
class NodePerformanceEvaluator:
    """Evaluates individual node performance for anchor selection"""
    
    def compute_loss_scores(self, logits, labels, mask):
        """Lower loss = higher performance score"""
        # Implementation planned
        
    def compute_confidence_scores(self, logits, mask):
        """Higher prediction confidence = higher score"""
        # Implementation planned
```

**DynamicAnchorRegularizer**:
```python
class DynamicAnchorRegularizer(nn.Module):
    """Integrates dynamic anchors with regularization framework"""
    
    def __init__(self, anchor_selector, fallback_config):
        self.anchor_selector = anchor_selector
        self.fallback_config = fallback_config
        
    def update_anchors(self, new_anchors):
        """Updates anchor references with soft update strategy"""
        # Implementation planned
        
    def compute_regularization_loss(self, embeddings):
        """Computes MMD/Wasserstein distance with current anchors"""
        # Implementation planned
```

### Existing Architecture (Retained)

#### Modular Design
```
core/              # Core utilities and configuration
â”œâ”€â”€ config.py      # Structured configuration management (enhanced for dynamic anchors)
â”œâ”€â”€ device.py      # Device management and optimization
â”œâ”€â”€ logging.py     # Enhanced logging with colors and experiment tracking
â”œâ”€â”€ reproducibility.py  # Comprehensive reproducibility utilities
â””â”€â”€ svd_reducer.py # Robust SVD feature reduction

models/            # Graph neural network architectures
â”œâ”€â”€ base.py        # Abstract base classes and interfaces
â””â”€â”€ architectures.py  # Concrete GNN implementations

datasets/          # Dataset loading and management
â”œâ”€â”€ base.py        # Abstract dataset interfaces
â”œâ”€â”€ loaders.py     # Concrete dataset loaders
â””â”€â”€ manager.py     # High-level dataset management with SVD integration

training/          # Training components
â”œâ”€â”€ losses.py      # Loss functions and target-centric losses
â”œâ”€â”€ regularizers.py # Target-centric regularization strategies (enhanced)
â”œâ”€â”€ augmentation.py # Graph data augmentation
â””â”€â”€ trainer.py     # Training loop abstractions (enhanced for dynamic anchors)
```

## Implementation Strategy

### Development Phases

#### Phase 1: Core Implementation (Weeks 1-2)
- [ ] Implement `DynamicPerformanceAnchorSelector` with basic loss-based selection
- [ ] Create `NodePerformanceEvaluator` for performance scoring
- [ ] Integrate with existing `TargetCentricRegularizer` framework
- [ ] Add configuration validation for dynamic anchor settings

#### Phase 2: Enhanced Features (Weeks 3-4)
- [ ] Add confidence-based scoring to selection criteria
- [ ] Implement anchor quality tracking and metrics
- [ ] Add soft update mechanism for training stability
- [ ] Develop comprehensive fallback system with MoG backup

#### Phase 3: Optimization & Validation (Weeks 5-6)
- [ ] Performance optimization for large graphs
- [ ] Comprehensive ablation studies on selection criteria
- [ ] Cross-domain validation across all dataset pairs
- [ ] Documentation and user guide completion

### Selection Criteria Implementation

#### Primary Criterion: Individual Node Loss
```python
def compute_loss_scores(logits, labels, mask):
    """
    Nodes with lower individual loss demonstrate better task alignment
    """
    node_losses = F.cross_entropy(logits, labels, reduction='none')  # [N]
    train_losses = node_losses[mask]
    
    # Invert and normalize: lower loss â†’ higher score
    min_loss, max_loss = train_losses.min(), train_losses.max()
    normalized = (train_losses - min_loss) / (max_loss - min_loss + 1e-8)
    scores = 1.0 - normalized
    
    return scores
```

#### Secondary Criterion: Prediction Confidence
```python
def compute_confidence_scores(logits, mask):
    """
    High confidence predictions indicate stable, reliable representations
    """
    probs = F.softmax(logits, dim=1)
    entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)
    confidence = -entropy  # Lower entropy = higher confidence
    
    train_confidence = confidence[mask]
    
    # Normalize to [0, 1] range
    min_conf, max_conf = train_confidence.min(), train_confidence.max()
    scores = (train_confidence - min_conf) / (max_conf - min_conf + 1e-8)
    
    return scores
```

## Performance Metrics & Validation

### Success Criteria

#### Primary Metrics
- **Downstream Task Accuracy**: 2-5% improvement over static MoG approach
- **Convergence Speed**: 10-20% fewer epochs to reach target performance
- **Cross-Domain Generalization**: Consistent improvements across dataset pairs

#### Quality Metrics
- **Anchor Diversity**: > 0.8 (normalized distance between anchors)
- **Representativeness**: < 0.5 (average distance from nodes to nearest anchor)
- **Selection Stability**: > 70% overlap between consecutive anchor updates

#### Computational Requirements
- **Training Overhead**: < 15% additional time compared to baseline
- **Memory Usage**: Minimal increase with efficient anchor storage
- **Scalability**: Support for graphs with 100k+ nodes

### Experimental Validation

#### Baseline Comparisons
1. **Static MoG Anchors**: Current implementation (primary baseline)
2. **Random Selection**: Random node selection as anchors
3. **Gaussian Prior**: Standard Gaussian anchor generation
4. **No Regularization**: Training without target-centric regularization

#### Test Scenarios
1. **Standard Cross-Domain**: All 20 dataset pair combinations
2. **Limited Labels**: 5-shot, 10-shot learning scenarios
3. **Large Graphs**: Scalability testing on Reddit, ogbn-products
4. **Heterogeneous Graphs**: Non-homophilic graph types

## Development Guidelines

### Adding Dynamic Anchor Components
1. Inherit from appropriate base classes in `core/` directory
2. Follow the established factory pattern for component creation
3. Implement comprehensive logging and error handling
4. Add configuration validation for new parameters
5. Include fallback mechanisms for robustness

### Configuration Management
- Use structured configuration with type validation
- Maintain backward compatibility with existing experiments
- Document all new parameters with clear descriptions
- Provide sensible defaults based on empirical validation

### Testing and Validation
- Implement unit tests for all anchor selection logic
- Add integration tests for training loop compatibility
- Include performance regression tests
- Validate across multiple random seeds for statistical significance

### Error Handling and Fallbacks
- Implement graceful degradation when dynamic selection fails
- Provide clear error messages with suggested solutions
- Log fallback activations for debugging
- Maintain training stability under all conditions

## Research Context

### Problem Statement
Traditional static anchor methods suffer from:
- **Fixed Reference Points**: Anchors don't adapt as model improves
- **Target-Only Modeling**: Logically inconsistent distribution modeling
- **Source Dependency**: Many methods require source domain access

### Innovation Summary
Dynamic Performance-Based Anchor Selection addresses these limitations by:
- Using actual task performance to identify high-quality nodes
- Continuously updating anchor selection as training progresses
- Operating in a completely source-free manner
- Leveraging target domain's intrinsic structure for reference generation

### Expected Research Impact
- **Immediate**: Improved cross-domain transfer performance
- **Medium-term**: New paradigm for adaptive regularization in graph learning
- **Long-term**: Foundation for self-supervised domain adaptation methods

---

## ðŸš€ IMPLEMENTATION PROGRESS & RESUME INSTRUCTIONS

### âœ… COMPLETED (Phase 1)
**Status**: Dynamic Anchor Selection Phase 1 implementation completed successfully!

#### Core Components Implemented:
1. **NodePerformanceEvaluator** (`core/performance_evaluator.py`)
   - Individual node performance scoring (loss + confidence)
   - Top-k node selection with configurable criteria
   - Comprehensive performance statistics

2. **DynamicPerformanceAnchorSelector** (`core/dynamic_anchor_selector.py`)
   - Performance-based dynamic anchor selection
   - Soft update mechanism for training stability
   - Quality tracking (diversity, representativeness, stability)
   - Selection history and statistics management

3. **DynamicAnchorRegularizer** (`training/dynamic_regularizers.py`)
   - Integration with existing divergence metrics (MMD, Wasserstein)
   - MoG fallback mechanism with automatic triggering
   - Performance monitoring and fallback condition checking
   - Comprehensive status tracking

4. **Enhanced TargetCentricLoss** (`training/losses.py`)
   - Dynamic anchor support with automatic selection
   - Backward compatibility with static methods
   - Epoch-aware anchor updates
   - Enhanced logging with anchor status information

#### Configuration & Integration:
- âœ… **config.yaml updated** with complete dynamic anchor configuration
- âœ… **Training script integration** (`train_prompt_tuning.py`) updated to pass epoch information
- âœ… **Import fixes** and path resolution completed
- âœ… **Comprehensive unit tests** created and passing (4/4 tests)

#### Verification Completed:
- âœ… All component imports working correctly
- âœ… Basic functionality tested with synthetic data
- âœ… Configuration loading and parsing verified
- âœ… Fallback mechanisms tested and working

### ðŸ“‹ NEXT STEPS TO RESUME

#### Phase 2: End-to-End Testing & Validation

1. **Integration Testing**:
   ```bash
   # Run integration tests (partially complete)
   python test_integration.py
   ```

2. **Small-Scale End-to-End Test**:
   ```bash
   # Test with small dataset (cora -> citeseer)
   python train_prompt_tuning.py
   ```

3. **Full Experimental Validation**:
   ```bash
   # Run complete cross-domain experiments
   python run_experiments.py
   ```

#### Configuration Status:
Current config.yaml settings:
- `dynamic_anchor.enable: true` (primary method)
- `target_centric.enable: false` (legacy disabled)
- Selection ratio: 20%, update frequency: 10 epochs
- MoG fallback enabled with conservative thresholds

#### Known Status:
- **All unit tests passing**: NodePerformanceEvaluator, DynamicAnchorSelector, DynamicRegularizer, TargetCentricLoss
- **Fallback mechanism active** in tests (expected behavior for synthetic data)
- **Training script modified** to support epoch-aware loss computation
- **Import issues resolved** with proper path handling

### ðŸ”§ IMMEDIATE RESUME ACTIONS

1. **Continue Integration Testing**:
   ```bash
   python test_integration.py  # Complete interrupted test
   ```

2. **Run First Real Experiment**:
   ```bash
   # Small test with real data
   python train_prompt_tuning.py
   ```

3. **Monitor and Debug**:
   - Check anchor selection behavior in real training
   - Verify fallback triggering conditions
   - Analyze performance vs baseline

### ðŸ“Š Expected Outcomes
- **2-5% accuracy improvement** over static MoG baseline
- **Dynamic anchor adaptation** during training
- **Robust fallback** when dynamic selection fails
- **Detailed logging** of anchor selection process

### ðŸš¨ Critical Notes for Next Session
- All core implementation is **COMPLETE** and **TESTED**
- Focus should be on **real data validation** and **performance analysis**
- Configuration is optimized for immediate testing
- Fallback mechanisms are conservative (will trigger easily for debugging)

---

**Implementation Status**: âœ… Phase 1 COMPLETE - Ready for End-to-End Testing
**Next Milestone**: Real dataset validation and performance comparison
**Estimated Time to Full Validation**: 1-2 hours